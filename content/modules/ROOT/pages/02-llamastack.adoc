= Deploying LlamaStack

Your cluster and namespace have a Custom Resource Definition (CRD) available for the Kind *LlamaStackDistribution* therefore you can create your own instance of a Llama Stack Server. 

First git clone a copy of the project files

[.console-input]
[source,bash,role="copypaste",subs=attributes+]
----
git clone https://github.com/burrsutter/fantaco-redhat-one-2026
cd fantaco-redhat-one-2026/
----

Review the Llama Stack Server manifest below and apply by copy and paste the command in the terminal in order to install a LlamaStack Distribution for vLLM using Model-as-a-Service (LiteMaaS):

[.console-input]
[source,bash,role="copypaste",subs=attributes+]
----
oc apply -f - <<'EOF'
apiVersion: llamastack.io/v1alpha1
kind: LlamaStackDistribution
metadata:
  name: llamastack-distribution-vllm
spec:
  replicas: 1
  server:
    distribution:
      name: starter
    containerSpec:
      env:
      - name: VLLM_URL
        value: "{{maas_api_url}}"
      - name: VLLM_API_TOKEN
        value: "{{maas_api_token}}"
    storage:
      size: "20Gi"
      mountPath: "/home/lls/.lls"
EOF
----

[.console-output]
[source,bash,subs=attributes+]
----
llamastackdistribution.llamastack.io/llamastack-distribution-vllm created
----

[.console-input]
[source,bash,role="copypaste",subs=attributes+]
----
oc get pods 
----

[.console-output]
[source,bash,subs=attributes+]
----
NAME                                            READY   STATUS    RESTARTS   AGE
llamastack-distribution-vllm-77897d9f8f-bzgbg   1/1     Running   0          4m22s
showroom-dd6dbbc4d-lq89g                        3/3     Running   0          17h
----

[.console-input]
[source,bash,role="copypaste",subs=attributes+]
----
oc get services
----

[.console-output]
[source,bash,subs=attributes+]
----
NAME                                   TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)    AGE
llamastack-distribution-vllm-service   ClusterIP   172.231.129.47   <none>        8321/TCP   5m18s
showroom                               ClusterIP   172.231.54.134   <none>        8080/TCP   17h
----

If you see the pod and service then congraluations, you have a successfully deployed Llama Stack.  In the next chapter, you will be exploring Llama Stack. 